# Plateforme de Gestion √âpicerie & Restaurant

Plateforme web compl√®te pour la gestion int√©gr√©e de deux m√©tiers : **√âpicerie**
(inventaire, catalogue, factures fournisseurs, suivi des prix) et **Restaurant**
(menus & recettes, co√ªts mati√®res, charges op√©rationnelles, marges). Un module
**Tr√©sorerie** unifi√© permet le suivi des flux bancaires, la cat√©gorisation
automatique des d√©penses et le rapprochement comptable.

L'architecture repose sur une **SPA React** moderne (Vite + TailwindCSS), une
**API REST FastAPI** et une base **PostgreSQL** avec support multi-tenant natif.

## √âtat du projet

* **Tests automatis√©s :** `pytest` couvre les services d'acc√®s aux donn√©es, le
  chargeur de produits, les extracteurs de factures ainsi que les conversions
  utilitaires utilis√©es par l'application principale.
* **Interface :** la feuille de style `legacy/streamlit/styles/style.css`
  applique une palette plus douce et chaleureuse √† l'ensemble des composants
  Streamlit, et le fichier
  `.streamlit/config.toml` force l'utilisation du th√®me clair sur tous les
  environnements d'ex√©cution.
* **SPA React :** le dossier `frontend/` contient une application Vite + React
  avec un router, un PoS minimal et d√©sormais deux exp√©riences : un shell
  complet pour **Epicerie¬†HQ** et une interface d√©di√©e, plus l√©g√®re, pour
  **Restaurant¬†HQ**.
* **Legacy archiv√© :** les anciennes applis Streamlit / PHP ont √©t√© d√©plac√©es
  sous `legacy/` et document√©es dans `docs/legacy.md`. Elles servent uniquement
  de r√©f√©rence (structures SQL, assets) et ne participent plus aux builds.
* **API REST :** `backend/main.py` expose un service FastAPI (`/health`,
  `/products`, `/inventory/summary`, `/pos/checkout`, `/products/{id}`) qui
  encapsule la logique m√©tier existante.
* **Workflows avanc√©s :** les onglets _Plan d'approvisionnement dynamique_,
  _Audit & r√©solution d'√©carts_, _Factures ‚Üí Commandes_, _Qualit√© catalogue &
  codes-barres_ et _Sauvegardes & reprise d'activit√©_ embarquent des vues
  orient√©es actions : calculs de couverture et propositions de commandes,
  assignation des √©carts avec export CSV, rapprochement factures / r√©ceptions,
  gouvernance des codes-barres et supervision des sauvegardes.

Pour v√©rifier localement que tout fonctionne, installez d'abord les
d√©pendances de d√©veloppement puis lancez la suite de tests :

```bash
pip install -r requirements-dev.txt
pytest
```

### Tests automatis√©s (API + SPA)

Un script permet d'orchestrer la base PostgreSQL de test, `pytest` et le build Vite :

```bash
./scripts/run-tests.sh
```

Il d√©marre un conteneur `db-test`, ex√©cute les migrations (`db/init.sql`), lance `pytest`
et v√©rifie le build front. Pensez √† arr√™ter les services r√©siduels avec
`docker compose --env-file .env.test down -v` en cas d'interruption.

## D√©marrer l'application

### V√©rifier votre version locale

Toutes les commandes `git` ci-dessous sont √† ex√©cuter dans un terminal ouvert
√† la racine du d√©p√¥t clon√© (le dossier `inventaire-epicerie`) **sans** les
pr√©fixer par `docker`. Elles fonctionnent aussi bien sur l'h√¥te que dans un
shell ouvert via `make shell`.

1. Afficher la branche active et l'√©tat des fichiers :

   ```bash
   git status -sb
   ```

2. Mettre √† jour les r√©f√©rences distantes puis comparer avec la branche
   distante suivie (ici `origin/work`) :

   ```bash
   git fetch origin
   git log --oneline HEAD..origin/work
   ```

3. Si vous souhaitez aligner votre copie locale sur la branche distante :

   ```bash
   git pull --ff-only
   ```

### Avec Docker (recommand√©)

1. Cr√©ez un fichier `.env` √† partir de `env.prod.example` en adaptant les
   valeurs si n√©cessaire.
2. Lancez la stack :

   ```bash
   docker compose up -d db api app frontend
   ```

3. D√®s que les conteneurs sont d√©marr√©s, ouvrez un navigateur sur
   <http://localhost:8501> pour acc√©der √† l'application Streamlit. La base
   PostgreSQL est expos√©e sur le port 5432 (d√©finis dans `docker-compose.yml`).
   L'API FastAPI est d√©sormais servie par le conteneur `inventaire-api` sur
   `http://localhost:8000`, ce qui permet de consommer l'API sans lancer
   `uvicorn` manuellement. Le front-end React est servi par le conteneur
   `inventaire-frontend` (build Vite) sur `http://localhost:5175`.
4. Pour arr√™ter et nettoyer les conteneurs :

   ```bash
   make down
   ```

#### R√©aligner la base existante

Les migrations Alembic supposaient l'existence de `capital_snapshot` et de la vue `latest_price_history`.
Si vous avez initialis√© la base avant ce correctif, cr√©ez-les manuellement puis resynchronisez la version Alembic¬†:

```bash
docker compose exec db psql -U postgres -d epicerie <<'SQL'
CREATE TABLE IF NOT EXISTS capital_snapshot (
  id SERIAL PRIMARY KEY,
  tenant_id INT NOT NULL,
  snapshot_date TIMESTAMPTZ NOT NULL,
  stock_value NUMERIC(14,2) NOT NULL DEFAULT 0,
  bank_balance NUMERIC(14,2) NOT NULL DEFAULT 0,
  cash_balance NUMERIC(14,2) NOT NULL DEFAULT 0,
  total_assets NUMERIC(14,2) NOT NULL DEFAULT 0,
  created_at TIMESTAMPTZ NOT NULL DEFAULT now()
);
CREATE INDEX IF NOT EXISTS idx_capital_snapshot_tenant_date ON capital_snapshot (tenant_id, snapshot_date);
CREATE OR REPLACE VIEW latest_price_history AS
SELECT code, tenant_id, fournisseur, prix_achat, quantite, facture_date, source_context, created_at
FROM (
  SELECT *,
         ROW_NUMBER() OVER (
           PARTITION BY tenant_id, code
           ORDER BY facture_date DESC NULLS LAST, created_at DESC
         ) AS row_num
  FROM produits_price_history
) ranked
WHERE row_num = 1;
SQL
docker compose exec api alembic stamp e32e12bd5139
docker compose restart api
```

Cela aligne la base de donn√©es avec le sch√©ma attendu par le portefeuille et √©vite les erreurs `UndefinedTable`.

#### Mettre √† jour le conteneur `app`

Lorsque vous modifiez le code Python ou les assets Streamlit, enregistrez vos
fichiers puis rechargez simplement la page : gr√¢ce au montage du dossier du
projet, la vue <http://localhost:8501> refl√®te imm√©diatement vos changements.

En revanche, les environnements sans montage (production, CI, export d'une
image) doivent √™tre reconstruits pour embarquer les nouveaux fichiers :

```bash
make rebuild          # reconstruit l'image puis relance les services
docker compose up --build app  # alternative √©quivalente
```

Cela garantit que le conteneur dispose bien des utilitaires comme
`invoice_extractor.py` ou `cart_normalizer.py`, et √©vite tout d√©calage entre
l'affichage local et l'image ex√©cut√©e en production.

### En local (hors Docker)

1. Cr√©ez et activez un environnement virtuel Python 3.11. Sur Debian/Ubuntu
   r√©cents (PEP¬†668), √©vitez l'option `--break-system-packages` et pr√©f√©rez un
   environnement isol√© :

   ```bash
   python3 -m venv .venv
   source .venv/bin/activate
   pip install --upgrade pip
   ```

2. Installez les d√©pendances applicatives et de test :

   ```bash
   pip install -r requirements-dev.txt
   ```

   (Ce fichier inclut `requirements.txt` et ajoute les outils de test comme
   `pytest`.)

3. Exportez les variables d'environnement n√©cessaires (voir `env.prod.example`
   pour la liste compl√®te) ou cr√©ez un fichier `.streamlit/secrets.toml`.

   En local, assurez-vous qu'au moins `DATABASE_URL` ou les variables
   `POSTGRES_*` individuelles (notamment `POSTGRES_PASSWORD`) sont d√©finies
   avant de lancer `uvicorn`, sinon PostgreSQL refuse la connexion avec
   `psycopg2.errors.OperationalError: fe_sendauth: no password supplied`.
4. D√©marrez l'application puis ouvrez votre navigateur sur
   <http://localhost:8501> :

   ```bash
   streamlit run legacy/streamlit/app.py
   ```

5. Dans un autre terminal, d√©marrez l'API puis la SPA :

   ```bash
   uvicorn backend.main:app --reload --port 8000
   cd frontend && npm install && npm run dev
   ```

   La SPA est disponible sur <http://localhost:5173> et communique avec
   l'ancienne application via l'iframe ¬´¬†Outils Streamlit¬†¬ª tant que certaines
   fonctionnalit√©s n'ont pas √©t√© port√©es. Si vous servez un build Vite derri√®re
   Streamlit, PHP ou Caddy sans proxy `/api`, d√©finissez `VITE_API_BASE_URL`
   (par exemple `http://localhost:8000`) avant `npm run build` pour que les
   exports CSV et appels REST ciblent directement FastAPI.

### Importer des produits

Un Makefile facilite l'import CSV :

```bash
make import-data
```

Par d√©faut, le fichier `docs/invoices/Produit.csv` sera charg√© et les codes barres seront
enregistr√©s. Red√©marrez ensuite l'application ou videz le cache Streamlit pour
voir les nouveaux produits.

### Importer automatiquement des factures METRO

Le script `scripts/import_invoice_files.py` orchestre l'int√©gralit√© du flux : extraction du PDF/DOCX, enrichissement catalogue, cr√©ation des mouvements de stock et alimentation des tables de consolidation (`dim_*`, `fact_*`).

```bash
source .venv/bin/activate
python scripts/import_invoice_files.py \
  --tenant epicerie \
  --supplier METRO \
  --username admin \
  docs/invoices/ilovepdf_merged-2.pdf docs/invoices/ilovepdf_merged-3.pdf
```

Options utiles : `--invoice-date YYYY-MM-DD` pour forcer la date de facture et `--initialize-stock` si vous souhaitez initialiser le stock lors de la cr√©ation d'un produit. Chaque import met automatiquement √† jour les tables `fact_invoices` ainsi que le catalogue (`produits`, `prix`, historiques).

### Sauvegardes PostgreSQL & onglet Maintenance

L'onglet **Maintenance (Admin)** de l'application affiche maintenant un
diagnostic des utilitaires PostgreSQL attendus (`pg_dump` et `psql`). Pour que
les boutons de sauvegarde/restauration fonctionnent :

1. Assurez-vous que le client PostgreSQL est install√© sur la machine qui
   ex√©cute Streamlit. Sous Debian/Ubuntu la commande suivante suffit en g√©n√©ral
   (ex√©cutez-la depuis l'h√¥te ou le conteneur concern√©) :

   ```bash
   sudo apt-get update
   sudo apt-get install postgresql-client
   ```

   Dans certains environnements, des variables `http_proxy`/`https_proxy`
   h√©rit√©es peuvent forcer l'utilisation d'un proxy inaccessible et provoquer
   des erreurs `403 Forbidden`. Relancez alors la commande en d√©sactivant ces
   variables :

   ```bash
   sudo env -u http_proxy -u https_proxy apt-get update
   sudo env -u http_proxy -u https_proxy apt-get install postgresql-client
   ```

2. V√©rifiez que l'utilisateur syst√®me qui ex√©cute Streamlit dispose des
   binaires dans son `PATH`. Si ce n'est pas le cas, configurez explicitement
   les chemins via les variables d'environnement `PG_DUMP_PATH` et `PSQL_PATH`
   (par exemple `/usr/lib/postgresql/16/bin/pg_dump`).

3. Une fois les utilitaires d√©tect√©s, l'onglet Maintenance expose :
   - le statut courant de chaque binaire (‚úÖ disponible ou ‚ùå introuvable),
   - la liste des sauvegardes existantes avec t√©l√©chargement, restauration ou
     suppression,
   - des messages d'aide si un pr√©requis manque.

Le dossier de sauvegarde par d√©faut est `backups/` (ou `/app/backups` en
production). Adaptez la variable `BACKUP_DIR` si besoin pour pointer vers un
volume persistant.

### Automatiser la cr√©ation de charges √† partir des relev√©s

Si vous importez des relev√©s bancaires en masse (PDF, texte, CSV), ex√©cutez ce
script pour rattacher automatiquement les lignes non encore trait√©es √† une
charge :

```bash
python -m scripts.auto_create_charges \
  --tenant 42 \
  --account "Compte restaurant" \
  [--limit 20] \
  [--dry-run]
```

- `--tenant` est l‚ÄôID du client (tenant) cibl√© ; il correspond √† la m√™me valeur
  que celle utilis√©e dans l‚Äôinterface.
- `--account` restreint le traitement √† un compte bancaire pr√©cis (facultatif).
- `--limit` tranche la quantit√© de relev√©s transform√©s en une seule passe.
- `--dry-run` affiche la liste des mouvements s√©lectionn√©s sans appeler
  l‚ÄôAPI de cr√©ation de charge.

La commande r√©utilise `restaurant_service.create_expense_from_bank_statement`
pour g√©n√©rer automatiquement une d√©pense par relev√© import√© et mettre √† jour
`depense_id`. En production, planifiez-la en cron (par exemple toutes les
nuit√©es) pour continuer √† synchroniser les relev√©s sans intervention manuelle.

### Explorer les alias de libell√©s bancaires

Pour mieux classifier les d√©penses, vous pouvez analyser les 2 derni√®res ann√©es
de relev√©s et rep√©rer les alias courts les plus fr√©quents :

```bash
python -m scripts.analyze_statement_names \
  --tenant 42 \
  --years 2 \
  --limit 20
```

Le script tronque chaque libell√© aux trois premiers mots (en majuscule) et affiche
les alias re√ßus le plus souvent. Cela permet de rep√©rer par exemple des enseignes
√† longue signature et de cr√©er des r√®gles `CATEGORY_RULES` plus fiables par regroupement.

### √âtendre automatique le dictionnaire de fournisseurs

Les r√®gles de reconnaissance s‚Äôappuient sur `data/vendor_category_mapping.csv` qui
liste les familles, les alias et les types correspondants. Ajoute une ligne par
tranche m√©tier (ex. `Banque / Finances`, `Approvisionnement`, `Plateformes`) avec :

1. `aliases`¬†: la liste des mots-cl√©s s√©par√©s par `|` utilis√©s dans les libell√©s.
2. `category`¬†: l‚Äô√©tiquette que tu veux voir appara√Ætre dans la vue restaurant.
3. `types`¬†: `Sortie`, `Entr√©e` ou rien si tu veux laisser la cat√©gorie sauvage.

√Ä chaque red√©marrage de l‚ÄôAPI, `backend/services/restaurant.py` charge automatiquement
les nouvelles familles via `core/vendor_categories.load_vendor_category_rules()`.

### G√©n√©rer le CSV depuis un tableau brut

Lorsque tu as un tableau complet (comme celui de ‚ÄúBanque / Finances‚Äù), colle-le tel quel dans un fichier texte (`banque.txt`, par exemple) avec une ligne par fournisseur et deux colonnes s√©par√©es par un tab ou deux espaces. Ensuite :

```bash
python -m scripts.import_vendor_list banque.txt
```

Le script d√©tecte les rubriques (`üßæ Banque / Finances`‚Ä¶), transforme chaque liste de fournisseurs en alias s√©par√©s par `|`, et ajoute les entr√©es dans `data/vendor_category_mapping.csv` (ajoute `--overwrite` pour repartir de z√©ro). C‚Äôest la mani√®re la plus rapide d‚Äôavoir tout ton lexique dans le CSV.

### R√©cat√©goriser les relev√©s existants

Les lignes d√©j√† import√©es restent avec leur ancienne cat√©gorie jusqu‚Äô√† ce que tu les
reclasses avec la nouvelle logique. Pour cela, lance la commande suivante c√¥t√© API :

```bash
python -m scripts/reclassify_bank_statements --tenant 42 [--account "Mon compte"] [--dry-run]
```

Utilise `--dry-run` pour v√©rifier le diff, puis relance sans l‚Äôoption pour appliquer les changements.

Les charges cr√©√©es automatiquement (`scripts.refresh_from_pdf` / bouton ‚ÄúCr√©er charge‚Äù) utilisent maintenant la m√™me cat√©gorie que les relev√©s gr√¢ce √† la correspondance `restaurant_depense_categories` (la cat√©gorie est cr√©√©e √† la vol√©e si elle n‚Äôexiste pas). Tu conserves ainsi une seule source de v√©rit√© entre Relev√©s et Charges.

Pour automatiser tout le flux (purge + import + charges + cat√©gories), tu peux aussi utiliser :

```bash
make refresh-reports PDF_DIR=/chemin/vers/tes/pdfs
```

Ce `make` target :
1. vide `restaurant_bank_statements` et `restaurant_depenses` via `psql` dans le service `db`,
2. r√©importe les PDF + cr√©e toutes les charges pour les relev√©s du compte,
3. recalcule les cat√©gories avec tes r√®gles les plus r√©centes.

D√©finis `PDF_DIR` pour pointer vers le dossier mont√© dans le conteneur `api` (par d√©faut `./pdfs` dans ton d√©p√¥t) et `TENANT_ID` pour l‚ÄôID num√©rique du tenant Restaurant (par d√©faut `1`).  
Cr√©e ce r√©pertoire et d√©pose-y les fichiers `.pdf` de relev√©s (ils deviennent accessibles √† `/app/$(notdir $(PDF_DIR))` dans le conteneur). Si le dossier est vide ou absent, la commande `make refresh-reports` √©choue et te le signale. Si ton tenant n‚Äôest pas `1`, red√©finis `TENANT_ID` pour la commande (ex. `make refresh-reports PDF_DIR=./pdfs TENANT_ID=42`).

### Orchestrer l‚Äôimport PDF complet

Pour automatiser compl√®tement la remise en route apr√®s une purge (PDF ‚Üí charge ‚Üí cat√©gories), utilise :

```bash
python -m scripts.refresh_from_pdf \
  --tenant restaurant \
  --account "incontournable" \
  --pdf path/to/releves \
  [--skip-charges] \
  [--skip-reclassify]
```

Le script :
1. balance tous les PDF fournis dans `restaurant_bank_statements` pour le compte choisi,
2. cr√©e les `restaurant_depenses` manquantes pour associer chaque ligne (comme `scripts.auto_create_charges.py`),
3. recalcule les cat√©gories avec les r√®gles √† jour (`core/vendor_categories`).

Les flags `--skip-charges` ou `--skip-reclassify` permettent d‚Äôignorer certaines √©tapes pour les tests rapides. Tu peux appeler cette commande apr√®s chaque lancement de batch PDF ou l‚Äôint√©grer dans un cron/docker-compose pour garder tout synchronis√©.

### R√©initialiser Restaurant HQ

Pour automatiser toute la s√©quence (chargement du `PARTIE_3_MENU.txt` + purge/import/reclassification), utilise le target suivant :

```bash
make reset-restaurant
```

Il encha√Æne `seed-partie3` (pour recharger ingr√©dients, boissons et cat√©gories) puis `refresh-reports` (pour vider les relev√©s et relancer le pipeline PDF ‚Üí charges ‚Üí cat√©gories). Tu peux le lancer apr√®s une purge compl√®te ou avant une nouvelle campagne d‚Äôimport de factures.

### Reconstituer l'historique des prix

Le module **Suivi prix** lit la table `produits_price_history`. Si elle est
vide, r√©importez vos factures via l'onglet _Factures ‚Üí Commandes_ ou appelez
directement `record_price_history` :

```python
import pandas as pd
from price_history_service import record_price_history

df = pd.read_csv("factures_2023_2024.csv")
record_price_history(
    df,
    supplier="Grossiste X",
    context="Reconstruction",
)
```

Chaque import ajoute les lignes avec prix d'achat, quantit√©s et date de facture,
ce qui permet d'alimenter les statistiques (min/max/moyenne) et les exports du
module **Suivi prix**.

### Nouvelles pages SPA (Rapports & Admin)

La SPA React (<http://localhost:5173>) int√®gre d√©sormais deux modules cl√©s qui
remplacent progressivement les vues Streamlit :

1. **Rapports consolid√©s (`/reports`)**
   - Consomme l'endpoint `GET /reports/overview` expos√© par FastAPI pour
     alimenter les KPI, la couverture par cat√©gorie, la rotation 30¬†jours, la
     liste des produits sous seuil et les stocks n√©gatifs.
   - Chaque bloc peut √™tre export√© en CSV via `GET /reports/export/{stock|alerts|rotation|negative_stock}`
     directement depuis l'interface (boutons ¬´¬†Exports CSV¬†¬ª).
   - Cette page suffit pour pr√©parer les revues catalogue sans lancer Streamlit :
     gardez simplement la stack `docker compose up -d db api app` en t√¢che de
     fond et servez la SPA (`npm run dev` ou `npm run build && npm run preview`).

2. **Outils administratifs (`/admin`)**
   - Reprend les diagnostics Streamlit (statut `pg_dump`/`psql`, planification,
     r√©tention, int√©grit√© des sauvegardes, comptes utilisateurs et √©carts de
     stock) via les nouveaux endpoints `/admin/*`.
   - Actions disponibles : cr√©ation/restauration/suppression de sauvegarde,
     mise √† jour de la planification (`PUT /admin/settings`), export du rapport
     d'int√©grit√© (`GET /admin/backups/integrity`), changement de r√¥le ou
     r√©initialisation de mot de passe utilisateur.
   - Les composants React d√©clenchent un rafra√Æchissement automatique du
     dashboard backend apr√®s chaque op√©ration, ce qui √©vite toute manipulation
     manuelle dans Streamlit.

Les outils historiques (scanner, extraction avanc√©e, etc.) restent accessibles
via l'onglet ¬´¬†Outils Streamlit¬†¬ª le temps de finaliser leur portage. Une fois
les pages _Rapports_ et _Admin_ valid√©es en production, il suffira de retirer
le conteneur `app` de la stack Docker pour exploiter uniquement FastAPI + SPA.
## Organisation du d√©p√¥t

- `backend/`, `frontend/`, `tests/`, `db/`, `migrations/`, `scripts/` :
  piles API/SPA, donn√©es et automatisations actuelles.
- `legacy/streamlit/` : application Streamlit originelle (script principal,
  pages, styles d√©di√©s, Dockerfile historique). Lancez-la avec
  `streamlit run legacy/streamlit/app.py`.
- `legacy/php/` : restes de l'ancienne vitrine PHP (pages marketing, assets,
  navigation ¬´¬†Customer¬†¬ª).
- `docs/restaurant/`, `docs/invoices/`, `docs/screenshots/` : r√©f√©rentiels
  fonctionnels (menus, BOM, cahiers des charges), exports fournisseurs et
  captures d'√©cran utilis√©s pendant la migration.

### Donn√©es de d√©mo Restaurant HQ

Pour visualiser imm√©diatement le cockpit Restaurant, un jeu de donn√©es YAML se
trouve dans `docs/restaurant/menu_seed.yaml`. Lancez simplement :

```
make seed-restaurant
```

ou directement :

```
python3 scripts/seed_restaurant.py --file docs/restaurant/menu_seed.yaml
```

Cela cr√©e/actualise les ingr√©dients, plats (avec fiches techniques) et quelques
charges fixes pour le tenant `restaurant`.

Les scripts SQL complets fournis par le m√©tier (RESTAURANT\_DEBUT\_MODELISATION,
BOM, charges, etc.) peuvent aussi √™tre appliqu√©s tels quels :

```
make seed-restaurant-sql
```

Si tu ne veux ins√©rer que les menus de `PARTIE_3_MENU.txt` (Bowl Manioc, Colombo
Gambas, etc.), utilise la cible sp√©cifique :

```
make seed-partie3
```

Et pour v√©rifier rapidement quels plats/ingr√©dients ont √©t√© inject√©s par
PARTIE_3_MENU (depuis l‚Äôenvironnement `.venv`), lance :

```
make check-partie3
```

Le fichier `docs/restaurant/menu_seed.yaml` inclut maintenant √©galement les plats
en sauce du relev√© (Heru, Ndole, Gombo, Jaune, Pistache, Arachide, Taro) avec leurs
ingr√©dients cl√©s, ce qui permet de recharger ces fiches techniques via
`make seed-restaurant` apr√®s avoir appliqu√© `seed-partie3`.

La page bancaire `BankStatementAnalyzer` contient deux comptes distincts :
¬´¬†Incontournable¬†¬ª (restaurant) et ¬´¬†Noutam¬†¬ª (√©picerie). Chaque ensemble reprend
les relev√©s issus des fichiers fournis (relev√©s Incontournable / Noutam) et
permet d‚Äôajouter manuellement de nouveaux relev√©s (coller du CSV ou glisser un
fichier). Il est maintenant possible d‚Äôenvoyer directement un PDF LCL
(`Importer un relev√© PDF`) ou d‚Äôutiliser la commande :

```
python3 scripts/import_bank_pdf.py --account incontournable ~/releves/
```

Le s√©lecteur de compte dans la page sert √† commuter entre les deux
instances.

La SPA Restaurant expose d√©sormais une page **Relev√©s bancaires** (route
`/bank-statement`) qui reprend ces transactions, fournit des filtres par mois et
cat√©gorie, et permet d‚Äôexporter le tableau filtr√© via
`frontend/src/features/restaurant/BankStatementAnalyzer.jsx`. Cette page
accepte aussi l‚Äôimport de relev√©s PDF LCL (upload direct)‚ÄØ: les fichiers sont
parseÃÅs c√¥t√© backend (`/restaurant/bank-statements/import-pdf`) et inject√©s
dans la table `restaurant_bank_statements`, pr√™ts √† √™tre corrig√©s ou enrichis
depuis l‚Äôinterface.

Pour v√©rifier les plats inject√©s, `python3 scripts/check_partie3.py` affiche la
liste des plats et ingr√©dients li√©s au tenant `restaurant`. Utile apr√®s
`seed-partie3` pour confirmer qu‚Äôon a bien appliqu√© la Partie¬†3.

Pour automatiser toutes les √©tapes (sch√©ma, SQL Restaurant, seed YAML) en une
seule commande :

```
make bootstrap-local
```

La cible appelle `python3 scripts/bootstrap_local.py` et prend les options
`--skip-schema`, `--skip-restaurant-sql`, `--skip-seed`, `--schema-file`,
`--restaurant-sql` et `--tenant` pour ma√Ætriser les phases ex√©cut√©es.

### D√©marrage complet local

`make start-dev` encha√Æne :
1. `python3 scripts/bootstrap_local.py --skip-restaurant-sql` pour appliquer le sch√©ma + le
   seed YAML.
2. `uvicorn backend.main:app --reload --port 8000`.
3. `cd frontend && npm run dev`.

Le script `scripts/start_dev_env.sh` g√®re la capture des signaux et arr√™te le
backend (uvicorn) quand `npm run dev` se termine.
### Pagination/filter catalogue

Le nouvel endpoint `GET /catalog/products` accepte d√©sormais les filtres `search`, `category`, `status` (`critical|warning|ok`) ainsi que `page` + `per_page`. Il retourne un objet `{ items: ProductOut[], meta: { page, per_page, total } }`.  
Tu peux l‚Äôutiliser pour construire les filtres dynamiques du Dashboard/Catalogue sans t√©l√©charger tout le catalogue en m√©moire (le `fetchProducts` c√¥t√© SPA accepte un objet `params`).
